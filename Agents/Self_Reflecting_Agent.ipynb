{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRS_XU4DrgjH"
      },
      "source": [
        "# üß† Production-Grade Self-Reflecting Agent with LangGraph\n",
        "\n",
        "This notebook demonstrates **how self-reflection is implemented in production agentic AI systems** using **LangGraph**.\n",
        "\n",
        "### Key characteristics\n",
        "- ‚úî LangGraph StateGraph\n",
        "- ‚úî No LLMs / No APIs\n",
        "- ‚úî Multiple strategies & models\n",
        "- ‚úî Accuracy vs cost trade-off\n",
        "- ‚úî Retry budget & termination logic\n",
        "- ‚úî Fully auditable agent memory\n",
        "\n",
        "This mirrors **real production agent loops**, not toy demos."
      ],
      "id": "fRS_XU4DrgjH"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdaWjXqPrgjI"
      },
      "source": [
        "## üß© Agent State (Production Schema)\n",
        "\n",
        "The state is shared across all LangGraph nodes.\n",
        "\n",
        "**Design goals:**\n",
        "- Explicit memory\n",
        "- Retry control\n",
        "- Strategy selection\n",
        "- Deterministic termination"
      ],
      "id": "TdaWjXqPrgjI"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "m5nWL7I8rgjJ"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass, field\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "@dataclass\n",
        "class AgentState:\n",
        "    history: List[Dict[str, Any]] = field(default_factory=list)\n",
        "    strategy: str = \"fast\"          # fast | accurate\n",
        "    retries: int = 0\n",
        "    max_retries: int = 3\n",
        "    done: bool = False"
      ],
      "id": "m5nWL7I8rgjJ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkzrAVyorgjK"
      },
      "source": [
        "## üì¶ Local ML Dependencies\n",
        "\n",
        "We intentionally use **simple, local models** to focus on **agent behavior**, not ML complexity."
      ],
      "id": "XkzrAVyorgjK"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fC1LXFfTrgjK"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "id": "fC1LXFfTrgjK"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2F_B7KOrgjL"
      },
      "source": [
        "## ‚öôÔ∏è LangGraph Action Node: Train Model\n",
        "\n",
        "This node represents an **agent action**.\n",
        "\n",
        "- `fast` strategy ‚Üí cheaper, faster model\n",
        "- `accurate` strategy ‚Üí more expensive, higher-quality model"
      ],
      "id": "f2F_B7KOrgjL"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "deoRWQIRrgjL"
      },
      "outputs": [],
      "source": [
        "def train_model_node(state: AgentState) -> AgentState:\n",
        "    X, y = make_classification(\n",
        "        n_samples=500,\n",
        "        n_features=10,\n",
        "        class_sep=0.8 if state.strategy == \"fast\" else 1.5,\n",
        "        flip_y=0.15,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "\n",
        "    if state.strategy == \"fast\":\n",
        "        model = LogisticRegression(max_iter=100)\n",
        "        cost = 1\n",
        "    else:\n",
        "        model = SVC(kernel=\"rbf\", gamma=\"scale\")\n",
        "        cost = 3\n",
        "\n",
        "    model.fit(X[:400], y[:400])\n",
        "    preds = model.predict(X[400:])\n",
        "    acc = accuracy_score(y[400:], preds)\n",
        "\n",
        "    state.history.append({\n",
        "        \"node\": \"train\",\n",
        "        \"strategy\": state.strategy,\n",
        "        \"accuracy\": acc,\n",
        "        \"cost\": cost\n",
        "    })\n",
        "\n",
        "    print(f\"[Train] strategy={state.strategy} | acc={acc:.2f} | cost={cost}\")\n",
        "    return state"
      ],
      "id": "deoRWQIRrgjL"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJxk2t54rgjL"
      },
      "source": [
        "## üîÅ LangGraph Reflection Node (Core Intelligence)\n",
        "\n",
        "This node **reads the agent‚Äôs own memory** and reasons over:\n",
        "\n",
        "- Accuracy\n",
        "- Compute cost\n",
        "- Retry budget\n",
        "\n",
        "It then decides whether to **retry with a better strategy or stop execution**."
      ],
      "id": "mJxk2t54rgjL"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "KYelHkVjrgjL"
      },
      "outputs": [],
      "source": [
        "def reflection_node(state: AgentState) -> AgentState:\n",
        "    last = state.history[-1]\n",
        "    acc = last[\"accuracy\"]\n",
        "    cost = last[\"cost\"]\n",
        "\n",
        "    print(f\"[Reflect] acc={acc:.2f} | cost={cost} | retries={state.retries}\")\n",
        "\n",
        "    if acc < 0.80 and state.retries < state.max_retries:\n",
        "        state.strategy = \"accurate\"\n",
        "        state.retries += 1\n",
        "        decision = \"retry_with_better_model\"\n",
        "    else:\n",
        "        state.done = True\n",
        "        decision = \"stop\"\n",
        "\n",
        "    state.history.append({\n",
        "        \"node\": \"reflect\",\n",
        "        \"decision\": decision,\n",
        "        \"next_strategy\": state.strategy,\n",
        "        \"done\": state.done\n",
        "    })\n",
        "\n",
        "    return state"
      ],
      "id": "KYelHkVjrgjL"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCxW7O-_rgjL"
      },
      "source": [
        "## üß≠ LangGraph Conditional Edge\n",
        "\n",
        "Reflection controls **graph execution flow**, not loops."
      ],
      "id": "yCxW7O-_rgjL"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "RLqdKu_yrgjL"
      },
      "outputs": [],
      "source": [
        "def should_continue(state: AgentState) -> str:\n",
        "    return \"end\" if state.done else \"train\""
      ],
      "id": "RLqdKu_yrgjL"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBgPJV_3rgjL"
      },
      "source": [
        "## üï∏Ô∏è Build the LangGraph (Production Pattern)"
      ],
      "id": "hBgPJV_3rgjL"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "iMHNyWjJrgjM"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "graph = StateGraph(AgentState)\n",
        "\n",
        "graph.add_node(\"train\", train_model_node)\n",
        "graph.add_node(\"reflect\", reflection_node)\n",
        "\n",
        "graph.set_entry_point(\"train\")\n",
        "graph.add_edge(\"train\", \"reflect\")\n",
        "\n",
        "graph.add_conditional_edges(\n",
        "    \"reflect\",\n",
        "    should_continue,\n",
        "    {\n",
        "        \"train\": \"train\",\n",
        "        \"end\": END\n",
        "    }\n",
        ")\n",
        "\n",
        "agent_graph = graph.compile()"
      ],
      "id": "iMHNyWjJrgjM"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZJ0Q80crgjM"
      },
      "source": [
        "## ‚ñ∂Ô∏è Run the Self-Reflecting Agent"
      ],
      "id": "NZJ0Q80crgjM"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b42ytNk3rgjM",
        "outputId": "b149a9e5-421c-44eb-e9bf-2a6b1c3f2fff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Train] strategy=fast | acc=0.73 | cost=1\n",
            "[Reflect] acc=0.73 | cost=1 | retries=0\n",
            "[Train] strategy=accurate | acc=0.81 | cost=3\n",
            "[Reflect] acc=0.81 | cost=3 | retries=1\n",
            "\n",
            "--- FINAL AGENT MEMORY ---\n",
            "{'node': 'train', 'strategy': 'fast', 'accuracy': 0.73, 'cost': 1}\n",
            "{'node': 'reflect', 'decision': 'retry_with_better_model', 'next_strategy': 'accurate', 'done': False}\n",
            "{'node': 'train', 'strategy': 'accurate', 'accuracy': 0.81, 'cost': 3}\n",
            "{'node': 'reflect', 'decision': 'stop', 'next_strategy': 'accurate', 'done': True}\n"
          ]
        }
      ],
      "source": [
        "final_state = agent_graph.invoke(AgentState())\n",
        "\n",
        "print(\"\\n--- FINAL AGENT MEMORY ---\")\n",
        "for h in final_state[\"history\"]:\n",
        "    print(h)"
      ],
      "id": "b42ytNk3rgjM"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhzqJDNNrgjM"
      },
      "source": [
        "## ‚úÖ Why This Is Production-Ready\n",
        "\n",
        "- ‚úî Explicit state & schema\n",
        "- ‚úî Deterministic execution\n",
        "- ‚úî Auditable memory trail\n",
        "- ‚úî Retry & cost control\n",
        "- ‚úî Reflection drives flow\n",
        "\n",
        "This pattern scales directly to:\n",
        "- LLM-as-judge reflection\n",
        "- Tool-using agents\n",
        "- RAG pipelines\n",
        "- Multi-agent systems"
      ],
      "id": "QhzqJDNNrgjM"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}