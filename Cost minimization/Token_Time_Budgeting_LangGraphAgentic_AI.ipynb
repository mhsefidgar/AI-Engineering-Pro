{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPqRLQgZ3siH"
      },
      "source": [
        "# Agentic Retrievalâ€‘Augmented Generation with LangGraph (Local LLM)\n",
        "\n",
        "This notebook demonstrates an **agentic RAG pipeline** using:\n",
        "- **LangGraph** as the agent orchestration framework\n",
        "- **FAISS** for semantic retrieval\n",
        "- A **local causal LLM** (`gpt2â€‘medium`) via HuggingFaceâ€™s `AutoModelForCausalLM`\n",
        "\n",
        "It includes **token budgeting**, **time budgeting**, and **loop avoidance** techniques."
      ],
      "id": "XPqRLQgZ3siH"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rsV8E_c23siJ",
        "outputId": "2735a3b6-77fa-41f4-b2f5-67a9afb49507"
      },
      "source": [
        "# Install dependencies\n",
        "!pip install langgraph langchain sentence-transformers faiss-cpu transformers"
      ],
      "id": "rsV8E_c23siJ",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langgraph in /usr/local/lib/python3.12/dist-packages (1.0.6)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (1.2.4)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.2.0)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.13.2-cp310-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.6)\n",
            "Requirement already satisfied: langchain-core>=0.1 in /usr/local/lib/python3.12/dist-packages (from langgraph) (1.2.7)\n",
            "Requirement already satisfied: langgraph-checkpoint<5.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (4.0.0)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langgraph) (1.0.6)\n",
            "Requirement already satisfied: langgraph-sdk<0.4.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (0.3.3)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langgraph) (2.12.3)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (3.6.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.9.0+cpu)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.36.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (0.6.4)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (9.1.2)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (0.13.0)\n",
            "Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<5.0.0,>=2.1.0->langgraph) (1.12.1)\n",
            "Requirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph) (3.11.5)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph) (0.4.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2026.1.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph) (4.12.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core>=0.1->langgraph) (3.0.0)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=0.1->langgraph) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=0.1->langgraph) (0.25.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
            "Downloading faiss_cpu-1.13.2-cp310-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (23.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m23.8/23.8 MB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.13.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPmxZDaK3siJ"
      },
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import faiss\n",
        "from typing import TypedDict, List\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# LangGraph imports\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.prebuilt import ToolNode\n",
        "\n",
        "# Setup local LLM and embeddings (no API cost)\n",
        "MODEL_NAME = 'gpt2-medium'\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
        "\n",
        "embed_model = SentenceTransformer('all-MiniLM-L6-v2')"
      ],
      "id": "XPmxZDaK3siJ",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNx18Y1I3siK"
      },
      "source": [
        "## Build Retrieval Index (FAISS)"
      ],
      "id": "HNx18Y1I3siK"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3oqutZoE3siK"
      },
      "source": [
        "# Doc corpus (example)\n",
        "documents = [\n",
        "    \"RAG agents combine retrieval with generative LLMs.\",\n",
        "    \"FAISS is a library for fast vector similarity search.\",\n",
        "    \"Token budgeting keeps processing within limits.\",\n",
        "    \"Time budgeting limits how long a step can run.\",\n",
        "    \"LangGraph orchestrates agentic workflows.\"\n",
        "]\n",
        "\n",
        "# Create embeddings and FAISS index\n",
        "doc_embeddings = embed_model.encode(documents)\n",
        "dim = doc_embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(dim)\n",
        "index.add(np.array(doc_embeddings))"
      ],
      "id": "3oqutZoE3siK",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEVjFEDA3siK"
      },
      "source": [
        "def retrieve_docs(query: str, top_k: int = 3) -> List[str]:\n",
        "    q_emb = embed_model.encode([query])\n",
        "    D, I = index.search(np.array(q_emb), top_k)\n",
        "    return [documents[i] for i in I[0]]"
      ],
      "id": "sEVjFEDA3siK",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "neXI-tMW3siK"
      },
      "source": [
        "def generate_local(prompt: str, max_tokens: int = 100) -> str:\n",
        "    inputs = tokenizer(prompt, return_tensors='pt')\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_tokens,\n",
        "        pad_token_id=tokenizer.eos_token_id  # prevent warnings\n",
        "    )\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
      ],
      "id": "neXI-tMW3siK",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osCeKuyT3siL"
      },
      "source": [
        "## Define LangGraph State\n",
        "\n",
        "Our state will keep track of the question, retrieved context, answers, retry count, and history to avoid loops."
      ],
      "id": "osCeKuyT3siL"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFJcJF2W3siL"
      },
      "source": [
        "class AgentState(TypedDict):\n",
        "    question: str\n",
        "    context: List[str]\n",
        "    answer: str\n",
        "    retry_count: int\n",
        "    history: List[str]\n",
        "\n",
        "# Budgets\n",
        "MAX_STEPS = 5\n",
        "MAX_TOTAL_TOKENS = 400\n",
        "MAX_STEP_TIME = 8  # seconds\n",
        "\n",
        "total_tokens = 0"
      ],
      "id": "JFJcJF2W3siL",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12nA6ZoR3siL"
      },
      "source": [
        "## Nodes (LangGraph)\n",
        "\n",
        "1. **retrieve_node:** Retrieves documents based on the question.\n",
        "2. **generate_node:** Generates an answer using local LLM.\n",
        "3. **decide_next:** Determines whether to continue looping or stop."
      ],
      "id": "12nA6ZoR3siL"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePQ5t5Ds3siL"
      },
      "source": [
        "def retrieve_node(state: AgentState) -> AgentState:\n",
        "    start = time.time()\n",
        "    docs = retrieve_docs(state['question'], top_k=3)\n",
        "    elapsed = time.time() - start\n",
        "    if elapsed > MAX_STEP_TIME:\n",
        "        state['context'] = []\n",
        "    else:\n",
        "        state['context'] = docs\n",
        "    return state  # always return state dict\n",
        "\n",
        "def generate_node(state: AgentState) -> AgentState:\n",
        "    global total_tokens\n",
        "    context = '\\n'.join(state['context'])\n",
        "    prompt = f\"Context: {context}\\nQ: {state['question']}\\nA:\"\n",
        "    start = time.time()\n",
        "    answer = generate_local(prompt, max_tokens=100)\n",
        "    elapsed = time.time() - start\n",
        "    if elapsed > MAX_STEP_TIME:\n",
        "        answer = \"(generation timed out)\"\n",
        "    # Token budgeting\n",
        "    toks = len(tokenizer.encode(answer))\n",
        "    total_tokens += toks\n",
        "    state['answer'] = answer\n",
        "    return state\n",
        "\n",
        "def decide_next_step(state: AgentState) -> AgentState:\n",
        "    # Increment retry\n",
        "    state['retry_count'] += 1\n",
        "\n",
        "    # Infinite loop detection using cosine similarity\n",
        "    ans_emb = embed_model.encode([state['answer']])\n",
        "    repeated = False\n",
        "    for prev in state['history']:\n",
        "        prev_emb = embed_model.encode([prev])\n",
        "        sim = np.dot(ans_emb, prev_emb.T) / (np.linalg.norm(ans_emb)*np.linalg.norm(prev_emb))\n",
        "        if sim > 0.9:\n",
        "            repeated = True\n",
        "            break\n",
        "\n",
        "    # Save to history\n",
        "    state['history'].append(state['answer'])\n",
        "\n",
        "    # Decide done flag\n",
        "    state['done'] = False\n",
        "    if repeated or state['retry_count'] >= MAX_STEPS or total_tokens >= MAX_TOTAL_TOKENS:\n",
        "        state['done'] = True\n",
        "\n",
        "    return state\n"
      ],
      "id": "ePQ5t5Ds3siL",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rh-wgp_33siL"
      },
      "source": [
        "## Build Graph\n"
      ],
      "id": "rh-wgp_33siL"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hky74DWN3siM"
      },
      "source": [
        "graph = StateGraph(AgentState)\n",
        "\n",
        "# Register nodes\n",
        "graph.add_node(\"retrieve_node\", retrieve_node)\n",
        "graph.add_node(\"generate_node\", generate_node)\n",
        "graph.add_node(\"decide_node\", decide_next_step)\n",
        "\n",
        "# Use string keys for start/end\n",
        "graph.add_edge(START, \"retrieve_node\")\n",
        "graph.add_edge(\"retrieve_node\", \"generate_node\")\n",
        "graph.add_edge(\"generate_node\", \"decide_node\")\n",
        "graph.add_edge(\"decide_node\", END)\n",
        "\n",
        "# Compile graph\n",
        "compiled = graph.compile()"
      ],
      "id": "hky74DWN3siM",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxRfTvKI3siM"
      },
      "source": [
        "## Invoke the Agent\n"
      ],
      "id": "AxRfTvKI3siM"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JE6qEnLg3siM",
        "outputId": "ed15ff4e-b1ee-47f1-da9b-e5348e1ff46f"
      },
      "source": [
        "initial_state = {\n",
        "    'question': \"Explain RAG agents in simple terms.\",\n",
        "    'context': [],\n",
        "    'answer': '',\n",
        "    'retry_count': 0,\n",
        "    'history': []\n",
        "}\n",
        "\n",
        "result = compiled.invoke(initial_state)\n",
        "\n",
        "print(\"Final Answer:\", result['answer'])\n",
        "print(\"Context Retrieved:\", result['context'])\n"
      ],
      "id": "JE6qEnLg3siM",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Answer: (generation timed out)\n",
            "Context Retrieved: ['RAG agents combine retrieval with generative LLMs.', 'LangGraph orchestrates agentic workflows.', 'FAISS is a library for fast vector similarity search.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrVg2Ai03siM"
      },
      "source": [
        "## ðŸ“Œ What This Notebook Implements\n",
        "- **Agentic workflow:** Your agent decides what to do next after retrieval and generation, without hardâ€‘coding a single linear chain.:contentReference[oaicite:1]{index=1}\n",
        "- **Token budgeting:** Track total tokens generated and stop when exceeded.\n",
        "- **Time budgeting:** Bound how long each step can run.\n",
        "- **Infinite loop prevention:** Use semantic similarity between answers to stop loops.\n",
        "- **Fully local execution:** Runs entirely without external API calls or internet dependencies."
      ],
      "id": "DrVg2Ai03siM"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}