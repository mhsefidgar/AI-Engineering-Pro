{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Cloths recomendation system design ðŸ¤– --> ðŸ‘š\n",
        "LangGraph and ollama 3.2 agentic AI system is used for cloth recomendation system."
      ],
      "metadata": {
        "id": "da3Q4K5nyjjf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_ollama"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8m_NZCO2nAq",
        "outputId": "f073aa30-658a-482c-e05c-07d99f1e9b15"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_ollama\n",
            "  Downloading langchain_ollama-1.0.1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain_ollama) (1.2.1)\n",
            "Collecting ollama<1.0.0,>=0.6.0 (from langchain_ollama)\n",
            "  Downloading ollama-0.6.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain_ollama) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain_ollama) (0.4.59)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain_ollama) (25.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain_ollama) (2.12.3)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain_ollama) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain_ollama) (9.1.2)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain_ollama) (4.15.0)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain_ollama) (0.12.0)\n",
            "Requirement already satisfied: httpx>=0.27 in /usr/local/lib/python3.12/dist-packages (from ollama<1.0.0,>=0.6.0->langchain_ollama) (0.28.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama<1.0.0,>=0.6.0->langchain_ollama) (4.12.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama<1.0.0,>=0.6.0->langchain_ollama) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama<1.0.0,>=0.6.0->langchain_ollama) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama<1.0.0,>=0.6.0->langchain_ollama) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27->ollama<1.0.0,>=0.6.0->langchain_ollama) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.0->langchain_ollama) (3.0.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain_ollama) (3.11.5)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain_ollama) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain_ollama) (2.32.4)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain_ollama) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.0->langchain_ollama) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.0->langchain_ollama) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.0->langchain_ollama) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain_ollama) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain_ollama) (2.5.0)\n",
            "Downloading langchain_ollama-1.0.1-py3-none-any.whl (29 kB)\n",
            "Downloading ollama-0.6.1-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: ollama, langchain_ollama\n",
            "Successfully installed langchain_ollama-1.0.1 ollama-0.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nMyXCasKyaK1",
        "outputId": "76ab8e6d-431a-4908-afb0-d27a1cc2d665"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing Ollama...\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading ollama-linux-amd64.tgz\n",
            "######################################################################## 100.0%\n",
            ">>> Creating ollama user...\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n"
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "import time\n",
        "import os\n",
        "\n",
        "# 1. Install Ollama (this step is blocking by nature)\n",
        "print(\"Installing Ollama...\")\n",
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Setting Up ollama chat langchain"
      ],
      "metadata": {
        "id": "N3WjgE_G08Dt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import time\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "\n",
        "# Necessary for Jupyter/Notebook environments\n",
        "nest_asyncio.apply()\n",
        "def setup_ollama():\n",
        "    # 1. Start the Ollama server in the background\n",
        "    print(\"Starting Ollama server...\")\n",
        "    ollama_log = open(\"ollama_server.log\", \"w\")\n",
        "\n",
        "    # Use start_new_session to ensure the server keeps running independently\n",
        "    subprocess.Popen(\n",
        "        [\"ollama\", \"serve\"],\n",
        "        stdout=ollama_log,\n",
        "        stderr=ollama_log,\n",
        "        start_new_session=True\n",
        "    )\n",
        "\n",
        "    # 2. Give the server a moment to initialize\n",
        "    time.sleep(5)\n",
        "\n",
        "    # 3. Pull the model (Blocking call to ensure it's ready before use)\n",
        "    print(\"Downloading Llama llama3.2... Please wait, this may take a few minutes.\")\n",
        "    try:\n",
        "        subprocess.run([\"ollama\", \"pull\", \"llama3.2\"], check=True)\n",
        "        print(\"âœ… Model downloaded and server is ready!\")\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"âŒ Error pulling model: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    setup_ollama()\n",
        "    # Your weather or LLM logic goes here"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZB_7HrQyjK0",
        "outputId": "f07b1388-0940-48b4-8799-58760cc9ce89"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Ollama server...\n",
            "Downloading Llama llama3.2... Please wait, this may take a few minutes.\n",
            "âœ… Model downloaded and server is ready!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Ollama example test"
      ],
      "metadata": {
        "id": "Vwr06r1g1W1v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from langchain_ollama import ChatOllama\n",
        "\n",
        "# Small model for fast response\n",
        "llm = ChatOllama(\n",
        "    model=\"llama3.2\",\n",
        "    temperature=0,\n",
        "    num_thread=4,\n",
        "    max_tokens=50\n",
        ")\n",
        "\n",
        "async def main():\n",
        "    answer = \"\"\n",
        "    print(\"Streaming response:\\n\")\n",
        "    async for chunk in llm.astream(\"What is the capital of Iran?\"):\n",
        "        # Each chunk has .content\n",
        "        print(chunk.content, end=\"\", flush=True)\n",
        "        answer += chunk.content\n",
        "\n",
        "    print(\"\\n\\nFinal answer:\", answer)\n",
        "\n",
        "await main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W3brZoW_1WRr",
        "outputId": "4df567c6-8010-4af2-fb5d-375203bf0236"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Streaming response:\n",
            "\n",
            "The capital of Iran is Tehran.\n",
            "\n",
            "Final answer: The capital of Iran is Tehran.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Setting up the weather APIs\n",
        "### First setup Geolocation finder"
      ],
      "metadata": {
        "id": "DqwyGxPG1BXC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from geopy.geocoders import Nominatim\n",
        "\n",
        "geolocator = Nominatim(user_agent=\"geoapi\")\n",
        "location = geolocator.geocode(\"Paris\")\n",
        "print(location.latitude, location.longitude)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9t-LHEA1Aq-",
        "outputId": "b415d1a5-4fd7-4172-8216-b8ffcc154f64"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "48.8588897 2.320041\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setting up weather fetcher API function"
      ],
      "metadata": {
        "id": "vu0sWD751JND"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "import httpx\n",
        "import time\n",
        "from typing import Dict\n",
        "from geopy.geocoders import Nominatim\n",
        "from geopy.exc import GeocoderTimedOut, GeocoderUnavailable\n",
        "\n",
        "# -----------------------------\n",
        "# Weather cache\n",
        "# -----------------------------\n",
        "WEATHER_CACHE = {}\n",
        "TTL_SECONDS = 600  # 10 minutes cache\n",
        "\n",
        "# -----------------------------\n",
        "# City coordinates cache\n",
        "# -----------------------------\n",
        "CITY_CACHE = {}\n",
        "\n",
        "# -----------------------------\n",
        "# Geopy geocoder\n",
        "# -----------------------------\n",
        "geolocator = Nominatim(user_agent=\"geoapi\")\n",
        "\n",
        "async def get_city_coords(city: str, max_retries=5) -> tuple:\n",
        "    \"\"\"Get city coordinates with retry, caching, and throttling\"\"\"\n",
        "    if city in CITY_CACHE:\n",
        "        return CITY_CACHE[city]\n",
        "\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            loop = asyncio.get_event_loop()\n",
        "            location = await loop.run_in_executor(None, lambda: geolocator.geocode(city, timeout=10))\n",
        "            if location:\n",
        "                coords = (location.latitude, location.longitude)\n",
        "                CITY_CACHE[city] = coords\n",
        "                # Respect Nominatim usage policy: 1 request per second\n",
        "                await asyncio.sleep(1)\n",
        "                return coords\n",
        "        except (GeocoderTimedOut, GeocoderUnavailable):\n",
        "            # exponential backoff\n",
        "            await asyncio.sleep(2 ** attempt)\n",
        "\n",
        "    raise ValueError(f\"City '{city}' could not be geocoded after {max_retries} retries!\")\n",
        "\n",
        "# -----------------------------\n",
        "# Async fetch weather\n",
        "# -----------------------------\n",
        "async def fetch_weather(lat: float, lon: float) -> Dict:\n",
        "    key = (lat, lon)\n",
        "    now = time.time()\n",
        "\n",
        "    if key in WEATHER_CACHE:\n",
        "        data, ts = WEATHER_CACHE[key]\n",
        "        if now - ts < TTL_SECONDS:\n",
        "            return data\n",
        "\n",
        "    url = f\"https://api.open-meteo.com/v1/forecast?latitude={lat}&longitude={lon}&current_weather=true\"\n",
        "    async with httpx.AsyncClient(timeout=10) as client:\n",
        "        resp = await client.get(url)\n",
        "        resp.raise_for_status()\n",
        "        data = resp.json()\n",
        "\n",
        "    WEATHER_CACHE[key] = (data, now)\n",
        "    return data\n",
        "\n",
        "# -----------------------------\n",
        "# Get weather + clothing advice\n",
        "# -----------------------------\n",
        "async def get_weather_and_clothing(city: str) -> Dict:\n",
        "    start_time = time.time()\n",
        "\n",
        "    lat, lon = await get_city_coords(city)\n",
        "    data = await fetch_weather(lat, lon)\n",
        "\n",
        "    current = data.get(\"current_weather\", {})\n",
        "    temp = current.get(\"temperature\", 20)\n",
        "    code = current.get(\"weathercode\", 0)\n",
        "\n",
        "    code_to_desc = {\n",
        "        0: \"Clear\", 1: \"Mainly clear\", 2: \"Partly cloudy\", 3: \"Overcast\",\n",
        "        45: \"Fog\", 48: \"Rime fog\", 51: \"Drizzle\", 53: \"Drizzle\", 55: \"Drizzle\",\n",
        "        61: \"Rain\", 63: \"Rain\", 65: \"Rain\", 71: \"Snow\", 73: \"Snow\", 75: \"Snow\",\n",
        "        80: \"Rain showers\", 81: \"Rain showers\", 82: \"Rain showers\", 95: \"Thunderstorm\",\n",
        "        96: \"Thunderstorm with hail\", 99: \"Thunderstorm with hail\"\n",
        "    }\n",
        "    description = code_to_desc.get(code, \"Clear\")\n",
        "\n",
        "    if temp < 5:\n",
        "        advice = \"Freezing! Heavy coat, thermal layers, and gloves.\"\n",
        "    elif 5 <= temp < 15:\n",
        "        advice = \"Chilly. Light jacket or sweater.\"\n",
        "    elif 15 <= temp < 22:\n",
        "        advice = \"Mild. Light sweater or denim jacket.\"\n",
        "    else:\n",
        "        advice = \"Warm. T-shirt and shorts.\"\n",
        "\n",
        "    if \"rain\" in description.lower():\n",
        "        advice += \" Bring an umbrella!\"\n",
        "\n",
        "    elapsed = time.time() - start_time\n",
        "    print(f\"[{city}] Fetched in {elapsed:.3f} sec\")\n",
        "\n",
        "    return {\n",
        "        \"city\": city,\n",
        "        \"temperature\": f\"{temp}Â°C\",\n",
        "        \"condition\": description,\n",
        "        \"advice\": advice\n",
        "    }\n",
        "\n",
        "# -----------------------------\n",
        "# Run for multiple cities sequentially to respect rate limits\n",
        "# -----------------------------\n",
        "async def main():\n",
        "    cities = [\"New York\", \"London\", \"Paris\", \"Toronto\", \"Berlin\", \"Tokyo\", \"Sydney\"]\n",
        "    results = []\n",
        "    for city in cities:\n",
        "        try:\n",
        "            results.append(await get_weather_and_clothing(city))\n",
        "        except ValueError as e:\n",
        "            print(e)\n",
        "\n",
        "    print(\"\\n--- Results ---\")\n",
        "    for r in results:\n",
        "        print(r)\n",
        "\n",
        "# -----------------------------\n",
        "# Run\n",
        "# -----------------------------\n",
        "await main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v60aQF031Jgh",
        "outputId": "70247311-8195-4625-bef8-8a52e2fb9072"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[New York] Fetched in 3.845 sec\n",
            "[London] Fetched in 3.427 sec\n",
            "[Paris] Fetched in 2.605 sec\n",
            "[Toronto] Fetched in 2.033 sec\n",
            "[Berlin] Fetched in 2.503 sec\n",
            "[Tokyo] Fetched in 2.105 sec\n",
            "[Sydney] Fetched in 2.732 sec\n",
            "\n",
            "--- Results ---\n",
            "{'city': 'New York', 'temperature': '7.5Â°C', 'condition': 'Overcast', 'advice': 'Chilly. Light jacket or sweater.'}\n",
            "{'city': 'London', 'temperature': '3.1Â°C', 'condition': 'Overcast', 'advice': 'Freezing! Heavy coat, thermal layers, and gloves.'}\n",
            "{'city': 'Paris', 'temperature': '4.4Â°C', 'condition': 'Overcast', 'advice': 'Freezing! Heavy coat, thermal layers, and gloves.'}\n",
            "{'city': 'Toronto', 'temperature': '3.2Â°C', 'condition': 'Overcast', 'advice': 'Freezing! Heavy coat, thermal layers, and gloves.'}\n",
            "{'city': 'Berlin', 'temperature': '-0.9Â°C', 'condition': 'Overcast', 'advice': 'Freezing! Heavy coat, thermal layers, and gloves.'}\n",
            "{'city': 'Tokyo', 'temperature': '12.4Â°C', 'condition': 'Mainly clear', 'advice': 'Chilly. Light jacket or sweater.'}\n",
            "{'city': 'Sydney', 'temperature': '20.6Â°C', 'condition': 'Partly cloudy', 'advice': 'Mild. Light sweater or denim jacket.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "import httpx\n",
        "import time\n",
        "from typing import Dict\n",
        "from geopy.geocoders import Nominatim\n",
        "from geopy.exc import GeocoderTimedOut, GeocoderUnavailable\n",
        "\n",
        "from langchain_core.tools import tool\n",
        "from langchain_core.messages import SystemMessage\n",
        "from langgraph.graph import StateGraph, MessagesState, START\n",
        "from langgraph.prebuilt import ToolNode, tools_condition\n",
        "from langchain_ollama import ChatOllama\n",
        "import nest_asyncio\n",
        "\n",
        "# -----------------------------\n",
        "# Notebook/async fix\n",
        "# -----------------------------\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# -----------------------------\n",
        "# Caches\n",
        "# -----------------------------\n",
        "WEATHER_CACHE = {}\n",
        "CITY_CACHE = {}\n",
        "TTL_SECONDS = 600  # 10 minutes cache\n",
        "\n",
        "# -----------------------------\n",
        "# Geopy geocoder\n",
        "# -----------------------------\n",
        "geolocator = Nominatim(user_agent=\"fashion_advisor\")\n",
        "\n",
        "async def get_city_coords(city: str, max_retries=5) -> tuple:\n",
        "    \"\"\"Get city coordinates with retry, caching, and throttling\"\"\"\n",
        "    if city in CITY_CACHE:\n",
        "        return CITY_CACHE[city]\n",
        "\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            loop = asyncio.get_event_loop()\n",
        "            location = await loop.run_in_executor(None, lambda: geolocator.geocode(city, timeout=10))\n",
        "            if location:\n",
        "                coords = (location.latitude, location.longitude)\n",
        "                CITY_CACHE[city] = coords\n",
        "                # Respect Nominatim usage policy: 1 request per second\n",
        "                await asyncio.sleep(1)\n",
        "                return coords\n",
        "        except (GeocoderTimedOut, GeocoderUnavailable):\n",
        "            await asyncio.sleep(2 ** attempt)\n",
        "\n",
        "    raise ValueError(f\"City '{city}' could not be geocoded after {max_retries} retries!\")\n",
        "\n",
        "# -----------------------------\n",
        "# Async fetch weather\n",
        "# -----------------------------\n",
        "async def fetch_weather(lat: float, lon: float) -> Dict:\n",
        "    key = (lat, lon)\n",
        "    now = time.time()\n",
        "\n",
        "    if key in WEATHER_CACHE:\n",
        "        data, ts = WEATHER_CACHE[key]\n",
        "        if now - ts < TTL_SECONDS:\n",
        "            return data\n",
        "\n",
        "    url = f\"https://api.open-meteo.com/v1/forecast?latitude={lat}&longitude={lon}&current_weather=true\"\n",
        "    async with httpx.AsyncClient(timeout=10) as client:\n",
        "        resp = await client.get(url)\n",
        "        resp.raise_for_status()\n",
        "        data = resp.json()\n",
        "\n",
        "    WEATHER_CACHE[key] = (data, now)\n",
        "    return data\n",
        "\n",
        "# -----------------------------\n",
        "# Weather + clothing advice tool\n",
        "# -----------------------------\n",
        "@tool\n",
        "async def get_weather_and_clothing(city: str) -> Dict:\n",
        "    \"\"\"\n",
        "    Fetch the current weather for a city and provide clothing advice\n",
        "    based on temperature and weather conditions.\n",
        "    \"\"\"\n",
        "    start_time = time.time()\n",
        "\n",
        "    lat, lon = await get_city_coords(city)\n",
        "    data = await fetch_weather(lat, lon)\n",
        "\n",
        "    current = data.get(\"current_weather\", {})\n",
        "    temp = current.get(\"temperature\", 20)\n",
        "    code = current.get(\"weathercode\", 0)\n",
        "\n",
        "    code_to_desc = {\n",
        "        0: \"Clear\", 1: \"Mainly clear\", 2: \"Partly cloudy\", 3: \"Overcast\",\n",
        "        45: \"Fog\", 48: \"Rime fog\", 51: \"Drizzle\", 53: \"Drizzle\", 55: \"Drizzle\",\n",
        "        61: \"Rain\", 63: \"Rain\", 65: \"Rain\", 71: \"Snow\", 73: \"Snow\", 75: \"Snow\",\n",
        "        80: \"Rain showers\", 81: \"Rain showers\", 82: \"Rain showers\", 95: \"Thunderstorm\",\n",
        "        96: \"Thunderstorm with hail\", 99: \"Thunderstorm with hail\"\n",
        "    }\n",
        "    description = code_to_desc.get(code, \"Clear\")\n",
        "\n",
        "    if temp < 5:\n",
        "        advice = \"Freezing! Heavy coat, thermal layers, and gloves.\"\n",
        "    elif 5 <= temp < 15:\n",
        "        advice = \"Chilly. Light jacket or sweater.\"\n",
        "    elif 15 <= temp < 22:\n",
        "        advice = \"Mild. Light sweater or denim jacket.\"\n",
        "    else:\n",
        "        advice = \"Warm. T-shirt and shorts.\"\n",
        "\n",
        "    if \"rain\" in description.lower():\n",
        "        advice += \" Bring an umbrella!\"\n",
        "\n",
        "    elapsed = time.time() - start_time\n",
        "    print(f\"[{city}] Fetched in {elapsed:.3f} sec\")\n",
        "\n",
        "    return {\n",
        "        \"city\": city,\n",
        "        \"temperature\": f\"{temp}Â°C\",\n",
        "        \"condition\": description,\n",
        "        \"advice\": advice\n",
        "    }\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# All tools\n",
        "# -----------------------------\n",
        "all_tools = [get_weather_and_clothing]\n",
        "\n",
        "# -----------------------------\n",
        "# LLM init\n",
        "# -----------------------------\n",
        "llm = ChatOllama(\n",
        "    model=\"llama3.2\",\n",
        "    temperature=0,\n",
        "    num_thread=4\n",
        ").bind_tools(all_tools)\n",
        "\n",
        "# -----------------------------\n",
        "# Graph setup\n",
        "# -----------------------------\n",
        "async def chatbot(state: MessagesState):\n",
        "    system_prompt = SystemMessage(content=(\n",
        "        \"You are a stylish Fashion Advisor. Use the 'get_weather_and_clothing' tool. \"\n",
        "        \"Take the base advice provided by the tool and turn it into a friendly, \"\n",
        "        \"high-end fashion recommendation for the user.  In plain English and brief pointing to the important tips\"\n",
        "    ))\n",
        "    messages = [system_prompt] + state[\"messages\"]\n",
        "    response = await llm.ainvoke(messages)\n",
        "    return {\"messages\": [response]}\n",
        "\n",
        "builder = StateGraph(MessagesState)\n",
        "builder.add_node(\"agent\", chatbot)\n",
        "builder.add_node(\"tools\", ToolNode(all_tools))\n",
        "\n",
        "builder.add_edge(START, \"agent\")\n",
        "builder.add_conditional_edges(\"agent\", tools_condition)\n",
        "builder.add_edge(\"tools\", \"agent\")\n",
        "\n",
        "graph = builder.compile()\n",
        "\n",
        "# -----------------------------\n",
        "# Execution example\n",
        "# -----------------------------\n",
        "async def main():\n",
        "    start = time.time()\n",
        "    cities = [\"Montreal\", \"London\", \"New York\"]\n",
        "    inputs = {\"messages\": [(\"user\", f\"I'm in {cities[0]}. What should I wear today?\")]}\n",
        "\n",
        "    async for event in graph.astream(inputs, stream_mode=\"values\"):\n",
        "        if \"messages\" in event:\n",
        "            last_message = event[\"messages\"][-1]\n",
        "            if last_message.type == \"ai\" and not last_message.tool_calls:\n",
        "                print(\"\\nADVISOR RESPONSE:\")\n",
        "                print(last_message.content)\n",
        "    end = time.time()\n",
        "    print('\\n'+ '*'* 300 +'\\n')\n",
        "    print(f\"Total response time: {end - start}\")\n",
        "'''\n",
        "    print(\"\\n--- Multi-city Weather Fetch ---\")\n",
        "    # Use .ainvoke with a dictionary for tools\n",
        "    results = await asyncio.gather(*(get_weather_and_clothing.ainvoke({\"city\": city}) for city in cities))\n",
        "    for r in results:\n",
        "        print(r)\n",
        "'''\n",
        "# -----------------------------\n",
        "# Run\n",
        "# -----------------------------\n",
        "await main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7KMmkUw2GWw",
        "outputId": "858498c1-8187-442b-f838-358f01c3301b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Montreal] Fetched in 2.347 sec\n",
            "\n",
            "ADVISOR RESPONSE:\n",
            "Bonjour! Given the current weather conditions in Montreal, I would recommend dressing warmly and stylishly for your day.\n",
            "\n",
            "First, start with a foundation of thermal layers - think long-sleeved tops and leggings or tights. This will help keep you cozy and warm despite the chilly temperature.\n",
            "\n",
            "Next, add a layer of protection from the elements with a heavy coat or parka. Look for one that's both stylish and functional, perhaps with a water-repellent finish to keep you dry in case of any unexpected rain showers.\n",
            "\n",
            "Don't forget to accessorize with some warm and toasty gloves - you can never go wrong with a pair of luxurious leather gloves to add a touch of sophistication to your outfit. And, of course, don't neglect a stylish hat or scarf to add a pop of color and texture to your look.\n",
            "\n",
            "Finally, consider adding some waterproof boots to complete your ensemble. Not only will they keep your feet dry, but they'll also add a chic touch to your overall style.\n",
            "\n",
            "Remember, dressing for the weather is all about layering up and choosing pieces that are both functional and fashionable. With these tips, you'll be well on your way to creating a stylish and warm outfit that's perfect for braving the Montreal winter!\n",
            "\n",
            "************************************************************************************************************************************************************************************************************************************************************************************************************\n",
            "\n",
            "Total response time: 6.8017964363098145\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##CongratulationðŸŽ‰ you have completed your course on how to use APIs in langGraph to make Agentic AI"
      ],
      "metadata": {
        "id": "YCSUYO_i4GQM"
      }
    }
  ]
}