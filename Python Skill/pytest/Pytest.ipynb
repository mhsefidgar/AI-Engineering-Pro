{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FiPOio9tpFVP"
      },
      "source": [
        "# ðŸ§ª Pytest Complete Tutorial\n",
        "\n",
        "This notebook is a **complete tutorial for pytest**, including:\n",
        "- Basic setup\n",
        "- Writing test functions\n",
        "- Assertions\n",
        "- Fixtures\n",
        "- Parametrization\n",
        "- Mocking\n",
        "- Running pytest in Jupyter\n",
        "- Best practices"
      ],
      "id": "FiPOio9tpFVP"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzZDm7ippFVQ"
      },
      "source": [
        "## 1. Installing pytest\n",
        "\n",
        "You can install pytest via pip:"
      ],
      "id": "LzZDm7ippFVQ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m4ZyXfOrpFVR",
        "outputId": "aad2b3ba-f9f1-4cf9-9da0-3790b0ced8b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytest in ./venv/lib/python3.12/site-packages (7.4.0)\n",
            "Requirement already satisfied: attrs>=19.2.0 in ./venv/lib/python3.12/site-packages (from pytest) (23.1.0)\n",
            "Requirement already satisfied: iniconfig in ./venv/lib/python3.12/site-packages (from pytest) (2.0.0)\n",
            "Requirement already satisfied: packaging in ./venv/lib/python3.12/site-packages (from pytest) (23.1)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in ./venv/lib/python3.12/site-packages (from pytest) (1.0.0)\n",
            "Requirement already satisfied: tomli>=1.0.0 in ./venv/lib/python3.12/site-packages (from pytest) (3.0.1)\n",
            "Requirement already satisfied: colorama in ./venv/lib/python3.12/site-packages (from pytest) (0.4.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install pytest"
      ],
      "id": "m4ZyXfOrpFVR"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "booSQTj-pFVS"
      },
      "source": [
        "## 2. Writing Your First Test\n",
        "\n",
        "Pytest discovers functions prefixed with `test_` in files named `test_*.py` or `*_test.py`. Let's create a simple function and test it."
      ],
      "id": "booSQTj-pFVS"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yCS2t6TmpFVS"
      },
      "outputs": [],
      "source": [
        "def add(a, b):\n",
        "    return a + b\n",
        "\n",
        "def test_add():\n",
        "    assert add(2, 3) == 5\n",
        "    assert add(-1, 1) == 0\n",
        "    assert add(0, 0) == 0"
      ],
      "id": "yCS2t6TmpFVS"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3zoPvyypFVT"
      },
      "source": [
        "## 3. Running Tests in Jupyter Notebook\n",
        "\n",
        "We can save the code to a file and run pytest:"
      ],
      "id": "Z3zoPvyypFVT"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81q0xL9XpFVT",
        "outputId": "aa908647-1ce5-4dec-afed-6e08fb2adc17"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================= test session starts =============================\n",
            "collected 1 item                                                                \n",
            "\n",
            "test_example.py::test_add PASSED                                          [100%]\n",
            "\n",
            "============================== 1 passed in 0.02s =============================\n"
          ]
        }
      ],
      "source": [
        "with open('test_example.py', 'w') as f:\n",
        "    f.write('''\n",
        "def add(a, b):\n",
        "    return a + b\n",
        "\n",
        "def test_add():\n",
        "    assert add(2, 3) == 5\n",
        "    assert add(-1, 1) == 0\n",
        "    assert add(0, 0) == 0\n",
        "''')\n",
        "\n",
        "!pytest -v test_example.py"
      ],
      "id": "81q0xL9XpFVT"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AstGO7R_pFVT"
      },
      "source": [
        "## 4. Assertions\n",
        "\n",
        "Pytest uses simple Python `assert` statements."
      ],
      "id": "AstGO7R_pFVT"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BThO4YcIpFVT"
      },
      "outputs": [],
      "source": [
        "def test_assertions():\n",
        "    # Equality\n",
        "    assert 2 + 2 == 4\n",
        "    # Boolean\n",
        "    assert True\n",
        "    # Membership\n",
        "    assert 'Py' in 'Pytest'\n",
        "    # Raises Exception\n",
        "    import math\n",
        "    try:\n",
        "        math.sqrt(-1)\n",
        "    except ValueError as e:\n",
        "        assert str(e) == 'math domain error'"
      ],
      "id": "BThO4YcIpFVT"
    },
    {
      "cell_type": "code",
      "source": [
        "with open('Assertions_test_example.py', 'w') as f:\n",
        "    f.write('''\n",
        "def test_assertions():\n",
        "    # Equality\n",
        "    assert 2 + 2 == 4\n",
        "    # Boolean\n",
        "    assert True\n",
        "    # Membership\n",
        "    assert 'Py' in 'Pytest'\n",
        "    # Raises Exception\n",
        "    import math\n",
        "    try:\n",
        "        math.sqrt(-1)\n",
        "    except ValueError as e:\n",
        "        assert str(e) == 'math domain error'\n",
        "''')\n",
        "\n",
        "!pytest -v Assertions_test_example.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71meT-tEpJcg",
        "outputId": "215fa2e4-eb8d-4548-ddc5-feac8285bf63"
      },
      "id": "71meT-tEpJcg",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux -- Python 3.12.12, pytest-8.4.2, pluggy-1.6.0 -- /usr/bin/python3\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content\n",
            "plugins: typeguard-4.4.4, langsmith-0.4.59, anyio-4.12.0\n",
            "\u001b[1mcollecting ... \u001b[0m\u001b[1m\rcollected 1 item                                                               \u001b[0m\n",
            "\n",
            "Assertions_test_example.py::test_assertions \u001b[32mPASSED\u001b[0m\u001b[32m                       [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.02s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-iA_3BhpFVU"
      },
      "source": [
        "## 5. Fixtures\n",
        "\n",
        "Fixtures help set up and clean up resources for tests."
      ],
      "id": "9-iA_3BhpFVU"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Y6bYS-VxpFVV"
      },
      "outputs": [],
      "source": [
        "import pytest\n",
        "\n",
        "@pytest.fixture\n",
        "def sample_list():\n",
        "    return [1, 2, 3]\n",
        "\n",
        "def test_fixture(sample_list):\n",
        "    assert sum(sample_list) == 6\n",
        "    sample_list.append(4)\n",
        "    assert sample_list[-1] == 4"
      ],
      "id": "Y6bYS-VxpFVV"
    },
    {
      "cell_type": "code",
      "source": [
        "with open('Fixture_test_example.py', 'w') as f:\n",
        "    f.write('''\n",
        "import pytest\n",
        "\n",
        "@pytest.fixture\n",
        "def sample_list():\n",
        "    return [1, 2, 3]\n",
        "\n",
        "def test_fixture(sample_list):\n",
        "    assert sum(sample_list) == 6\n",
        "    sample_list.append(4)\n",
        "    assert sample_list[-1] == 4\n",
        "''')\n",
        "\n",
        "!pytest -v Fixture_test_example.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_LUZDxTNpV01",
        "outputId": "5e4c7355-e159-4cca-f1d9-4e1465df0a34"
      },
      "id": "_LUZDxTNpV01",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux -- Python 3.12.12, pytest-8.4.2, pluggy-1.6.0 -- /usr/bin/python3\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content\n",
            "plugins: typeguard-4.4.4, langsmith-0.4.59, anyio-4.12.0\n",
            "\u001b[1mcollecting ... \u001b[0m\u001b[1m\rcollected 1 item                                                               \u001b[0m\n",
            "\n",
            "Fixture_test_example.py::test_fixture \u001b[32mPASSED\u001b[0m\u001b[32m                             [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wX-SeM4SpFVV"
      },
      "source": [
        "## 6. Parametrized Tests\n",
        "\n",
        "Run the same test with multiple inputs using `@pytest.mark.parametrize`."
      ],
      "id": "wX-SeM4SpFVV"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "OLTdWABBpFVW"
      },
      "outputs": [],
      "source": [
        "@pytest.mark.parametrize(\"a,b,expected\", [\n",
        "    (2, 3, 5),\n",
        "    (-1, 1, 0),\n",
        "    (0, 0, 0)\n",
        "])\n",
        "def test_add_param(a, b, expected):\n",
        "    assert add(a, b) == expected"
      ],
      "id": "OLTdWABBpFVW"
    },
    {
      "cell_type": "code",
      "source": [
        "with open('Parametrized_test_example.py', 'w') as f:\n",
        "    f.write('''\n",
        "import pytest\n",
        "\n",
        "@pytest.fixture\n",
        "def sample_list():\n",
        "    return [1, 2, 3]\n",
        "\n",
        "def test_fixture(sample_list):\n",
        "    assert sum(sample_list) == 6\n",
        "    sample_list.append(4)\n",
        "    assert sample_list[-1] == 4\n",
        "''')\n",
        "\n",
        "!pytest -v Parametrized_test_example.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-2IGJLDYpevU",
        "outputId": "586b1d8a-4bdd-41b1-f67a-d256c01f2e67"
      },
      "id": "-2IGJLDYpevU",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux -- Python 3.12.12, pytest-8.4.2, pluggy-1.6.0 -- /usr/bin/python3\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content\n",
            "plugins: typeguard-4.4.4, langsmith-0.4.59, anyio-4.12.0\n",
            "\u001b[1mcollecting ... \u001b[0m\u001b[1m\rcollected 1 item                                                               \u001b[0m\n",
            "\n",
            "Parametrized_test_example.py::test_fixture \u001b[32mPASSED\u001b[0m\u001b[32m                        [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXPcFgkbpFVW"
      },
      "source": [
        "## 7. Testing Exceptions\n",
        "\n",
        "Use `pytest.raises` to assert exceptions."
      ],
      "id": "vXPcFgkbpFVW"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "SagItsghpFVW"
      },
      "outputs": [],
      "source": [
        "def divide(a, b):\n",
        "    return a / b\n",
        "\n",
        "def test_divide_zero():\n",
        "    with pytest.raises(ZeroDivisionError):\n",
        "        divide(1, 0)"
      ],
      "id": "SagItsghpFVW"
    },
    {
      "cell_type": "code",
      "source": [
        "with open('Exceptions_test_example.py', 'w') as f:\n",
        "    f.write('''\n",
        "import pytest\n",
        "\n",
        "@pytest.fixture\n",
        "def sample_list():\n",
        "    return [1, 2, 3]\n",
        "\n",
        "def test_fixture(sample_list):\n",
        "    assert sum(sample_list) == 6\n",
        "    sample_list.append(4)\n",
        "    assert sample_list[-1] == 4\n",
        "''')\n",
        "\n",
        "!pytest -v Exceptions_test_example.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jwzJ5W4JpnUS",
        "outputId": "66a1f038-d25f-41ee-b0f4-37b46336b014"
      },
      "id": "jwzJ5W4JpnUS",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux -- Python 3.12.12, pytest-8.4.2, pluggy-1.6.0 -- /usr/bin/python3\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content\n",
            "plugins: typeguard-4.4.4, langsmith-0.4.59, anyio-4.12.0\n",
            "\u001b[1mcollecting ... \u001b[0m\u001b[1m\rcollected 1 item                                                               \u001b[0m\n",
            "\n",
            "Exceptions_test_example.py::test_fixture \u001b[32mPASSED\u001b[0m\u001b[32m                          [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.02s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIPq6rkypFVW"
      },
      "source": [
        "## 8. Skipping and Expected Failures\n",
        "\n",
        "You can skip tests or mark expected failures."
      ],
      "id": "kIPq6rkypFVW"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "TKev2I7mpFVX"
      },
      "outputs": [],
      "source": [
        "@pytest.mark.skip(reason=\"Not implemented yet\")\n",
        "def test_skip():\n",
        "    assert False\n",
        "\n",
        "@pytest.mark.xfail(reason=\"This test will fail\")\n",
        "def test_expected_fail():\n",
        "    assert 1 == 2"
      ],
      "id": "TKev2I7mpFVX"
    },
    {
      "cell_type": "code",
      "source": [
        "with open('Skipping_and_Expected_test_example.py', 'w') as f:\n",
        "    f.write('''\n",
        "@pytest.mark.skip(reason=\"Not implemented yet\")\n",
        "def test_skip():\n",
        "    assert False\n",
        "\n",
        "@pytest.mark.xfail(reason=\"This test will fail\")\n",
        "def test_expected_fail():\n",
        "    assert 1 == 2\n",
        "''')\n",
        "\n",
        "!pytest -v Exceptions_test_example.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0LugsG0bpx0J",
        "outputId": "9ff8d643-680a-4b23-84e4-9fe06341c8d3"
      },
      "id": "0LugsG0bpx0J",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux -- Python 3.12.12, pytest-8.4.2, pluggy-1.6.0 -- /usr/bin/python3\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content\n",
            "plugins: typeguard-4.4.4, langsmith-0.4.59, anyio-4.12.0\n",
            "\u001b[1mcollecting ... \u001b[0m\u001b[1m\rcollected 1 item                                                               \u001b[0m\n",
            "\n",
            "Exceptions_test_example.py::test_fixture \u001b[32mPASSED\u001b[0m\u001b[32m                          [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECvxF_RWpFVX"
      },
      "source": [
        "## 9. Mocking\n",
        "\n",
        "Use `unittest.mock` to replace parts of your system during tests."
      ],
      "id": "ECvxF_RWpFVX"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "LlyncgS9pFVX"
      },
      "outputs": [],
      "source": [
        "from unittest.mock import Mock\n",
        "\n",
        "def test_mock():\n",
        "    mock_obj = Mock()\n",
        "    mock_obj.method.return_value = 42\n",
        "    assert mock_obj.method() == 42\n",
        "    mock_obj.method.assert_called_once()"
      ],
      "id": "LlyncgS9pFVX"
    },
    {
      "cell_type": "code",
      "source": [
        "with open('Mocking_test_example.py', 'w') as f:\n",
        "    f.write('''\n",
        "from unittest.mock import Mock\n",
        "\n",
        "def test_mock():\n",
        "    mock_obj = Mock()\n",
        "    mock_obj.method.return_value = 42\n",
        "    assert mock_obj.method() == 42\n",
        "    mock_obj.method.assert_called_once()\n",
        "''')\n",
        "\n",
        "!pytest -v Mocking_test_example.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPNlNsfUqOxn",
        "outputId": "0eed444b-3252-4e42-c423-bea0163ae237"
      },
      "id": "ZPNlNsfUqOxn",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux -- Python 3.12.12, pytest-8.4.2, pluggy-1.6.0 -- /usr/bin/python3\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content\n",
            "plugins: typeguard-4.4.4, langsmith-0.4.59, anyio-4.12.0\n",
            "\u001b[1mcollecting ... \u001b[0m\u001b[1m\rcollected 1 item                                                               \u001b[0m\n",
            "\n",
            "Mocking_test_example.py::test_mock \u001b[32mPASSED\u001b[0m\u001b[32m                                [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMoFhp_cpFVX"
      },
      "source": [
        "## 10. Best Practices\n",
        "\n",
        "- Keep test functions small and focused.\n",
        "- Use fixtures for repeated setup.\n",
        "- Name test files `test_*.py`.\n",
        "- Use `pytest.mark.parametrize` for multiple inputs.\n",
        "- Use `pytest.raises` for exceptions.\n",
        "- Keep tests independent of each other.\n",
        "- Use mocking for external dependencies."
      ],
      "id": "RMoFhp_cpFVX"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7Sg6ennpFVX"
      },
      "source": [
        "## 11. Running Tests Options\n",
        "\n",
        "- Run all tests: `pytest`\n",
        "- Verbose mode: `pytest -v`\n",
        "- Run a specific test file: `pytest test_example.py`\n",
        "- Run a specific test function: `pytest -k test_add`\n",
        "- Show print statements: `pytest -s`"
      ],
      "id": "p7Sg6ennpFVX"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhRFNHpapFVX"
      },
      "source": [
        "ðŸŽ‰ **Congratulations!** You now have a full, productive `pytest` tutorial in a single notebook compatible with Python 3.12."
      ],
      "id": "YhRFNHpapFVX"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}