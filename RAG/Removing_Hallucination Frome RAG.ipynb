{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Hallucination removal techniques from LLMs"
      ],
      "metadata": {
        "id": "728Epkvo3pf6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## In this notebook we focus on the how to remove hallucination from an LLM"
      ],
      "metadata": {
        "id": "ru4IM3t33yzF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Cross encoder reranking"
      ],
      "metadata": {
        "id": "vXfoWnmp04Ut"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZePlirrN0zL-",
        "outputId": "4d3b61d2-5acb-4d19-f77c-7b8dd2d5bffc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reranked documents:\n",
            "- Cross-Encoders are used for reranking search results.\n",
            "- RAG pipelines can use Cross-Encoders for precise scoring.\n",
            "- Bi-Encoders encode queries and documents separately.\n"
          ]
        }
      ],
      "source": [
        "from sentence_transformers import CrossEncoder\n",
        "\n",
        "# 1. Load a pre-trained Cross-Encoder model\n",
        "# Example: 'cross-encoder/ms-marco-MiniLM-L-6-v2'\n",
        "model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
        "\n",
        "# 2. Define your query and candidate documents\n",
        "query = \"What is Cross-Encoder used for?\"\n",
        "candidates = [\n",
        "    \"Cross-Encoders are used for reranking search results.\",\n",
        "    \"Bi-Encoders encode queries and documents separately.\",\n",
        "    \"RAG pipelines can use Cross-Encoders for precise scoring.\"\n",
        "]\n",
        "\n",
        "# 3. Prepare input pairs (query, document)\n",
        "pairs = [(query, doc) for doc in candidates]\n",
        "\n",
        "# 4. Compute relevance scores\n",
        "scores = model.predict(pairs, show_progress_bar=False)\n",
        "\n",
        "# 5. Rerank documents by score (descending)\n",
        "reranked = [doc for _, doc in sorted(zip(scores, candidates), reverse=True)]\n",
        "\n",
        "# 6. Output\n",
        "print(\"Reranked documents:\")\n",
        "for doc in reranked:\n",
        "    print(\"-\", doc)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Dense Passage Retrieval (DPR)"
      ],
      "metadata": {
        "id": "bkwaSv5s1Gzb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer, DPRContextEncoder, DPRContextEncoderTokenizer\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# 1. Load DPR encoders and tokenizers\n",
        "query_encoder = DPRQuestionEncoder.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n",
        "query_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n",
        "\n",
        "context_encoder = DPRContextEncoder.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
        "context_tokenizer = DPRContextEncoderTokenizer.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
        "\n",
        "# 2. Example query and passages\n",
        "query = \"What is Dense Passage Retrieval?\"\n",
        "passages = [\n",
        "    \"Dense Passage Retrieval is a bi-encoder based retrieval method.\",\n",
        "    \"Cross-Encoders are used for reranking candidate documents.\",\n",
        "    \"DPR encodes queries and passages into dense vectors for fast search.\"\n",
        "]\n",
        "\n",
        "# 3. Encode query\n",
        "query_inputs = query_tokenizer(query, return_tensors=\"pt\")\n",
        "query_vec = query_encoder(**query_inputs).pooler_output  # [1, hidden_size]\n",
        "\n",
        "# 4. Encode passages\n",
        "context_inputs = context_tokenizer(\n",
        "    passages,\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=256,  # truncate long passages\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "#context_inputs = context_tokenizer(passages, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "context_vecs = context_encoder(**context_inputs).pooler_output  # [num_passages, hidden_size]\n",
        "\n",
        "# 5. Compute cosine similarity\n",
        "scores = F.cosine_similarity(query_vec, context_vecs)\n",
        "top_k = 2\n",
        "top_results = torch.topk(scores, top_k)\n",
        "\n",
        "print(\"Top passages:\")\n",
        "for score, idx in zip(top_results.values, top_results.indices):\n",
        "    print(f\"{passages[idx]} (score: {score.item():.4f})\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9he26tea1FDz",
        "outputId": "567eead5-c7fe-4b00-b8aa-955d5121493e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at facebook/dpr-question_encoder-single-nq-base were not used when initializing DPRQuestionEncoder: ['question_encoder.bert_model.pooler.dense.bias', 'question_encoder.bert_model.pooler.dense.weight']\n",
            "- This IS expected if you are initializing DPRQuestionEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DPRQuestionEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at facebook/dpr-ctx_encoder-single-nq-base were not used when initializing DPRContextEncoder: ['ctx_encoder.bert_model.pooler.dense.bias', 'ctx_encoder.bert_model.pooler.dense.weight']\n",
            "- This IS expected if you are initializing DPRContextEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DPRContextEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \n",
            "The class this function is called from is 'DPRContextEncoderTokenizer'.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top passages:\n",
            "Dense Passage Retrieval is a bi-encoder based retrieval method. (score: 0.7644)\n",
            "DPR encodes queries and passages into dense vectors for fast search. (score: 0.6054)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RAFT --> Retrival Augmented FineTuning"
      ],
      "metadata": {
        "id": "Fo9BRajD2Kf-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from torch.optim import AdamW\n",
        "\n",
        "# ======== 1. Setup ========\n",
        "model_name = \"gpt2\"  # pretrained LLM\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "# Tiny toy dataset\n",
        "training_data = [\n",
        "    {\n",
        "        \"query\": \"What is RAFT?\",\n",
        "        \"target_answer\": \"RAFT fine-tunes LLMs using retrieved documents.\",\n",
        "        \"retrieved_docs\": [\"RAFT improves LLM accuracy by leveraging retrieved knowledge.\"]\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"What is DPR?\",\n",
        "        \"target_answer\": \"DPR is a dense retriever for passages.\",\n",
        "        \"retrieved_docs\": [\"DPR encodes queries and passages into dense vectors.\"]\n",
        "    }\n",
        "]\n",
        "\n",
        "# ======== 2. RAFT Fine-Tuning Loop ========\n",
        "model.train()  # set model to training mode\n",
        "\n",
        "for example in training_data:\n",
        "    query = example[\"query\"]\n",
        "    target_answer = example[\"target_answer\"]\n",
        "    retrieved_docs = example[\"retrieved_docs\"]\n",
        "\n",
        "    # 2a. Combine query + retrieved docs + target answer\n",
        "    context_text = f\"Query: {query}\\nContext: {' '.join(retrieved_docs)}\\nAnswer: \"\n",
        "    full_text = context_text + target_answer\n",
        "\n",
        "    # 2b. Tokenize full input\n",
        "    inputs = tokenizer(full_text, return_tensors=\"pt\")\n",
        "\n",
        "    # 2c. Create labels\n",
        "    labels = inputs.input_ids.clone()\n",
        "    # Mask out query + context so loss is only computed on target answer\n",
        "    answer_start = len(tokenizer(context_text).input_ids)\n",
        "    labels[0, :answer_start] = -100  # -100 means ignore\n",
        "\n",
        "    # 2d. Forward pass → RAFT fine-tuning happens here\n",
        "    outputs = model(**inputs, labels=labels)\n",
        "    loss = outputs.loss\n",
        "    print(f\"Training loss: {loss.item():.4f}\")\n",
        "\n",
        "    # 2e. Backpropagation → update weights\n",
        "    loss.backward()          # <-- RAFT fine-tuning step\n",
        "    optimizer.step()         # <-- RAFT fine-tuning step\n",
        "    optimizer.zero_grad()    # reset gradients for next example\n",
        "\n",
        "# ======== 3. Inference ========\n",
        "model.eval()  # set model to eval mode\n",
        "query = \"Explain RAFT.\"\n",
        "retrieved_docs = [\"RAFT fine-tunes LLMs using retrieved documents.\"]\n",
        "context_text = f\"Query: {query}\\nContext: {' '.join(retrieved_docs)}\\nAnswer: \"\n",
        "inputs = tokenizer(context_text, return_tensors=\"pt\")\n",
        "\n",
        "outputs = model.generate(**inputs, max_length=50)\n",
        "print(\"Generated answer:\", tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VssHubHp3Ck5",
        "outputId": "5780edfd-673c-4624-a388-98fbeda59462"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss: 4.6769\n",
            "Training loss: 4.6720\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated answer: Query: Explain RAFT.\n",
            "Context: RAFT fine-tunes LLMs using retrieved documents.\n",
            "Answer:  The RAFT document is a document that is retrieved from the RAFT database.  The document is a document that\n"
          ]
        }
      ]
    }
  ]
}